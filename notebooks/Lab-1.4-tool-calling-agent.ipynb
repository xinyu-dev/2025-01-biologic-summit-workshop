{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.4 Tool Calling Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool-calling ability of LLMs refers to their capability to interact with external systems, APIs, or tools during their execution to perform specific tasks or augment their responses with functionalities beyond their internal knowledge. In this notebook we will look at how to use a simple tool with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models that support tool calling\n",
    "\n",
    "\n",
    "In general, openAI models (e.g. `gpt-4o-mini`) as well as certain LLM NIMs (e.g. `llama-3.1-405b-instruct`) support tool calling. To find out which LLM NIM support tool calling, we can use the function below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "# requires NVIDIA_API_KEY in .env\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/workshop/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:176: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "tool_models = [\n",
    "    model for model in ChatNVIDIA.get_available_models() if model.supports_tools\n",
    "]\n",
    "tool_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic example of tool-calling agent in LangGraph\n",
    "\n",
    "Now we can look at a basic example of tool calling agent in LangGraph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: LLM NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# initialize the LLM client\n",
    "model = ChatNVIDIA(\n",
    "  model=\"meta/llama-3.2-3b-instruct\",\n",
    "  api_key=os.getenv(\"NVIDIA_NIM_API_KEY\"), \n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: OpenAI model (try it outside the workshop)\n",
    "\n",
    "> OpenAI model requires an API key. This is not covered in the workshop, but we provide snippets so that you can try it out on your own. \n",
    "\n",
    "\n",
    "Go to `2025-01-biologic-summit-workshop/.env` file, and uncomment this line: \n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=\"YOUR OPENAI API KEY HERE\"\n",
    "```\n",
    "Put your openAI API key, so that it can loaded into environment variable. Then run the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.tools import tool\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def magic_function_1(input: int) -> int:\n",
    "    \"\"\"Applies a magic function 1 to an input.\"\"\"\n",
    "    return input * 100\n",
    "\n",
    "@tool\n",
    "def magic_function_2(input: int) -> int:\n",
    "    \"\"\"Applies a magic function 2 to an input.\"\"\"\n",
    "    return input/100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the tool to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can \n",
    "tools = [magic_function_1, magic_function_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant. Repsond truthfully.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# create session_id\n",
    "session_id = \"session_1\"\n",
    "\n",
    "# create a memory saver\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(\n",
    "    model, tools, state_modifier=system_message, checkpointer=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out\n",
    "\n",
    "In the conversation the user asks a qeustion first, then the agent respond. This counts as 1 conversation turn. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the magic function with an input of 2 is 200.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the result of magic function with input of 2\"\n",
    "messages = langgraph_agent_executor.invoke(\n",
    "    {\"messages\": [(\"human\", query)]}, \n",
    "    config= {\"configurable\": {\"thread_id\": session_id}}\n",
    ")\n",
    "\n",
    "print(messages[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if the user asks another question, this counts as the 2nd turn. This is a good time to check the memory, to see if the agent has remembered the previous conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked: \"What is the result of magic function with input of 2\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2nd turn\n",
    "query = \"What's the question I just asked?\"\n",
    "messages = langgraph_agent_executor.invoke(\n",
    "    {\"messages\": [(\"human\", query)]}, \n",
    "    config= {\"configurable\": {\"thread_id\": session_id}}\n",
    ")\n",
    "print(messages[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test to see if the agent can differentiate magic function 1 and 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the magic function 2 with an input of 2 is 0.02.\n"
     ]
    }
   ],
   "source": [
    "# 3rd turn\n",
    "query = \"What's the result of magic function 2 with input of 2?\"\n",
    "messages = langgraph_agent_executor.invoke(\n",
    "    {\"messages\": [(\"human\", query)]}, \n",
    "    config= {\"configurable\": {\"thread_id\": session_id}}\n",
    ")\n",
    "print(messages[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here you can see that the agent has remembered the previous conversation, and correctly applied the correct tool. \n",
    "\n",
    "Please proceed to Lab 1.5. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
